{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a00a8eb",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment 1: Boolean Model, TF-IDF, and Data Retrieval vs. Information Retrieval Conceptual Questions\n",
    "\n",
    "**Student names**: _Your_names_here_ <br>\n",
    "**Group number**: _Your_group_here_ <br>\n",
    "**Date**: _Submission Date_\n",
    "\n",
    "## Important notes\n",
    "Please carefully read the following notes and consider them for the assignment delivery. Submissions that do not fulfill these requirements will not be assessed and should be submitted again.\n",
    "1. You may work in groups of maximum 2 students.\n",
    "2. The assignment must be delivered in ipynb format.\n",
    "3. The assignment must be typed. Handwritten assignments are not accepted.\n",
    "\n",
    "**Due date**: 14.09.2025 23:59\n",
    "\n",
    "In this assignment, you will:\n",
    "- Implement a Boolean retrieval model\n",
    "- Compute TF-IDF vectors for documents\n",
    "- Run retrieval on queries\n",
    "- Answer conceptual questions \n",
    "\n",
    "---\n",
    "## Dataset\n",
    "\n",
    "You will use the **Cranfield** dataset, provided in this file:\n",
    "\n",
    "- `cran.all.1400`: The document collection (1400 documents)\n",
    "\n",
    "**The code to parse the file is ready — just update the cran file path to match your own file location. Use the docs variable in your code for the parsed file**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3249058",
   "metadata": {},
   "source": [
    "### Load and parse documents (provided)\n",
    "\n",
    "Run the cell to parse the Cranfield documents. Update the path so it points to your `cran.all.1400` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773d293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 1400 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read 'cran.all.1400' and parse the documents into a suitable data structure\n",
    "\n",
    "CRAN_PATH = r\"cran.all.1400\"  # <-- change this!\n",
    "\n",
    "def parse_cranfield(path):\n",
    "    docs = {}\n",
    "    current_id = None\n",
    "    current_field = None\n",
    "    buffers = {\"T\": [], \"A\": [], \"B\": [], \"W\": []}\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if line.startswith(\".I \"):\n",
    "                if current_id is not None:\n",
    "                    docs[current_id] = {\n",
    "                        \"id\": current_id,\n",
    "                        \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "                        \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "                    }\n",
    "                current_id = int(line.split()[1])\n",
    "                buffers = {k: [] for k in buffers}\n",
    "                current_field = None\n",
    "            elif line.startswith(\".\"):\n",
    "                tag = line[1:].strip()\n",
    "                current_field = tag if tag in buffers else None\n",
    "            else:\n",
    "                if current_field is not None:\n",
    "                    buffers[current_field].append(line)\n",
    "    if current_id is not None:\n",
    "        docs[current_id] = {\n",
    "            \"id\": current_id,\n",
    "            \"title\": \" \".join(buffers[\"T\"]).strip(),\n",
    "            \"abstract\": \" \".join(buffers[\"W\"]).strip()\n",
    "        }\n",
    "    print(f\"Parsed {len(docs)} documents.\")\n",
    "    return docs\n",
    "\n",
    "docs = parse_cranfield(CRAN_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8f900",
   "metadata": {},
   "source": [
    "## 1.1 – Boolean Retrieval Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81516f89",
   "metadata": {},
   "source": [
    "### 1.1.1 Tokenize documents\n",
    "\n",
    "Implement tokenization using the given list of stopwords. Create a list of normalized terms per document (e.g., lowercase, remove punctuation/digits; drop stopwords). Store the token lists to use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78a135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 1400 documents\n",
      "\n",
      "Example tokenization for document 1:\n",
      "Original title: experimental investigation of the aerodynamics of a wing in a slipstream .\n",
      "Original abstract: experimental investigation of the aerodynamics of a wing in a slipstream .   an experimental study o...\n",
      "Tokenized terms: ['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution']...\n",
      "Total terms in this document: 84\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement tokenization using the given list of stopwords, create list of terms per document\n",
    "\n",
    "import re\n",
    "\n",
    "STOPWORDS = set(\"\"\"a about above after again against all am an and any are aren't as at be because been\n",
    "before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down\n",
    "during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers\n",
    "herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most\n",
    "mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she\n",
    "she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's\n",
    "these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're\n",
    "we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't\n",
    "you you'd you'll you're you've your yours yourself yourselves\"\"\".split())\n",
    "\n",
    "def tokenize_document(text):\n",
    "    \"\"\"\n",
    "    Tokenize a document by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing punctuation and digits\n",
    "    3. Splitting into tokens\n",
    "    4. Removing stopwords\n",
    "    5. Returning list of normalized terms\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and digits, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Split into tokens and remove empty strings\n",
    "    tokens = [token.strip() for token in text.split() if token.strip()]\n",
    "    \n",
    "    # Remove stopwords and return filtered tokens\n",
    "    filtered_tokens = [token for token in tokens if token not in STOPWORDS]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Tokenize all documents\n",
    "tokenized_docs = {}\n",
    "for doc_id, doc in docs.items():\n",
    "    # Combine title and abstract for tokenization\n",
    "    combined_text = f\"{doc['title']} {doc['abstract']}\"\n",
    "    tokenized_docs[doc_id] = tokenize_document(combined_text)\n",
    "\n",
    "# Display some examples\n",
    "print(f\"Tokenized {len(tokenized_docs)} documents\")\n",
    "print(f\"\\nExample tokenization for document {list(docs.keys())[0]}:\")\n",
    "print(f\"Original title: {docs[list(docs.keys())[0]]['title']}\")\n",
    "print(f\"Original abstract: {docs[list(docs.keys())[0]]['abstract'][:100]}...\")\n",
    "print(f\"Tokenized terms: {tokenized_docs[list(docs.keys())[0]][:20]}...\")\n",
    "print(f\"Total terms in this document: {len(tokenized_docs[list(docs.keys())[0]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183f40d",
   "metadata": {},
   "source": [
    "### Build vocabulary\n",
    "\n",
    "Create a set (or list) of unique terms from all tokenized documents. Report the number of unique terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9cc192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique terms in vocabulary: 6934\n",
      "Total number of documents processed: 1400\n",
      "Total terms across all documents: 141060\n",
      "Average terms per document: 100.76\n",
      "\n",
      "First 20 terms in vocabulary (alphabetically):\n",
      "['ab', 'abbreviated', 'ability', 'ablated', 'ablating', 'ablation', 'ablative', 'able', 'abrupt', 'abruptly', 'absence', 'absent', 'absolute', 'absorbed', 'absorbing', 'absorption', 'abstract', 'abundantly', 'academic', 'accelerated']\n",
      "\n",
      "Vocabulary statistics:\n",
      "- Shortest term length: 1\n",
      "- Longest term length: 21\n",
      "- Average term length: 7.91\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a set or list of unique terms\n",
    "\n",
    "# Create vocabulary from all tokenized documents\n",
    "vocabulary = set()\n",
    "\n",
    "# Collect all unique terms from all documents\n",
    "for doc_id, terms in tokenized_docs.items():\n",
    "    vocabulary.update(terms)\n",
    "\n",
    "# Convert to sorted list for easier handling\n",
    "vocabulary_list = sorted(list(vocabulary))\n",
    "\n",
    "# Report: \n",
    "# - Number of unique terms\n",
    "print(f\"Number of unique terms in vocabulary: {len(vocabulary_list)}\")\n",
    "print(f\"Total number of documents processed: {len(tokenized_docs)}\")\n",
    "\n",
    "# Show some statistics\n",
    "total_terms = sum(len(terms) for terms in tokenized_docs.values())\n",
    "avg_terms_per_doc = total_terms / len(tokenized_docs)\n",
    "\n",
    "print(f\"Total terms across all documents: {total_terms}\")\n",
    "print(f\"Average terms per document: {avg_terms_per_doc:.2f}\")\n",
    "\n",
    "# Show first 20 terms as examples\n",
    "print(f\"\\nFirst 20 terms in vocabulary (alphabetically):\")\n",
    "print(vocabulary_list[:20])\n",
    "\n",
    "# Show some interesting statistics\n",
    "print(f\"\\nVocabulary statistics:\")\n",
    "print(f\"- Shortest term length: {min(len(term) for term in vocabulary_list)}\")\n",
    "print(f\"- Longest term length: {max(len(term) for term in vocabulary_list)}\")\n",
    "print(f\"- Average term length: {sum(len(term) for term in vocabulary_list) / len(vocabulary_list):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb02912",
   "metadata": {},
   "source": [
    "### Build inverted index\n",
    "\n",
    "For each term, store the list (or set) of document IDs where the term appears.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393b2683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index built successfully!\n",
      "Number of terms in inverted index: 6934\n",
      "Total vocabulary size: 6934\n",
      "\n",
      "Example entries from inverted index:\n",
      "'experimental': appears in 318 documents -> [1, 11, 12, 17, 19, 25, 29, 30, 35, 41]...\n",
      "'investigation': appears in 216 documents -> [1, 8, 9, 19, 29, 30, 44, 45, 50, 56]...\n",
      "'aerodynamics': appears in 24 documents -> [1, 11, 33, 216, 225, 237, 244, 284, 289, 296]...\n",
      "'wing': appears in 181 documents -> [1, 13, 14, 30, 31, 42, 52, 60, 69, 76]...\n",
      "'slipstream': appears in 14 documents -> [1, 409, 453, 484, 1064, 1089, 1090, 1091, 1092, 1094]...\n",
      "\n",
      "Terms with highest document frequency:\n",
      "'flow': appears in 702 documents\n",
      "'results': appears in 596 documents\n",
      "'pressure': appears in 520 documents\n",
      "'number': appears in 484 documents\n",
      "'boundary': appears in 459 documents\n",
      "\n",
      "Terms with lowest document frequency (appearing in only 1 document):\n",
      "Number of terms appearing in only 1 document: 2669\n",
      "Examples: ['libby', 'wassermann', 'contaminates', 'persist', 'ensuing', 'phosphorescent', 'lacquer', 'rake', 'hastening', 'performances']\n",
      "\n",
      "Average document frequency: 13.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: For each term, store list of document IDs where the term appears\n",
    "\n",
    "# Build inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# For each document, add its ID to the posting list of each term it contains\n",
    "for doc_id, terms in tokenized_docs.items():\n",
    "    for term in terms:\n",
    "        if term not in inverted_index:\n",
    "            inverted_index[term] = []\n",
    "        inverted_index[term].append(doc_id)\n",
    "\n",
    "# Sort document IDs for each term (for consistency and easier processing)\n",
    "for term in inverted_index:\n",
    "    inverted_index[term] = sorted(list(set(inverted_index[term])))\n",
    "\n",
    "# Report statistics\n",
    "print(f\"Inverted index built successfully!\")\n",
    "print(f\"Number of terms in inverted index: {len(inverted_index)}\")\n",
    "print(f\"Total vocabulary size: {len(vocabulary_list)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nExample entries from inverted index:\")\n",
    "example_terms = list(inverted_index.keys())[:5]\n",
    "for term in example_terms:\n",
    "    doc_list = inverted_index[term]\n",
    "    print(f\"'{term}': appears in {len(doc_list)} documents -> {doc_list[:10]}{'...' if len(doc_list) > 10 else ''}\")\n",
    "\n",
    "# Find terms with highest and lowest document frequencies\n",
    "term_frequencies = [(term, len(doc_list)) for term, doc_list in inverted_index.items()]\n",
    "term_frequencies.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTerms with highest document frequency:\")\n",
    "for term, freq in term_frequencies[:5]:\n",
    "    print(f\"'{term}': appears in {freq} documents\")\n",
    "\n",
    "print(f\"\\nTerms with lowest document frequency (appearing in only 1 document):\")\n",
    "single_doc_terms = [term for term, freq in term_frequencies if freq == 1]\n",
    "print(f\"Number of terms appearing in only 1 document: {len(single_doc_terms)}\")\n",
    "if single_doc_terms:\n",
    "    print(f\"Examples: {single_doc_terms[:10]}\")\n",
    "\n",
    "# Calculate average document frequency\n",
    "avg_doc_freq = sum(len(doc_list) for doc_list in inverted_index.values()) / len(inverted_index)\n",
    "print(f\"\\nAverage document frequency: {avg_doc_freq:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e81bf",
   "metadata": {},
   "source": [
    "### Retrieve documents for a Boolean query (AND/OR)\n",
    "\n",
    "Create a function to retrieve documents for a Boolean query (AND/OR) with query terms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c9318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function for retrieving documents for a Boolean query (AND/OR) with query terms\n",
    "\n",
    "def boolean_retrieve(query: str):\n",
    "    \"\"\"\n",
    "    Retrieve documents for a Boolean query with AND/OR operations.\n",
    "    \n",
    "    Args:\n",
    "        query: String containing terms and Boolean operators (AND/OR)\n",
    "               Example: \"gas AND pressure\" or \"structural AND aeroelastic OR flight\"\n",
    "    \n",
    "    Returns:\n",
    "        List of document IDs that match the query\n",
    "    \"\"\"\n",
    "    # Tokenize the query using the same tokenization function\n",
    "    query_terms = tokenize_document(query)\n",
    "    \n",
    "    if not query_terms:\n",
    "        return []\n",
    "    \n",
    "    # Parse the query to separate terms and operators\n",
    "    # Split by AND/OR while preserving the operators\n",
    "    import re\n",
    "    \n",
    "    # Split query by AND/OR operators, keeping the operators\n",
    "    parts = re.split(r'\\s+(AND|OR)\\s+', query.upper())\n",
    "    \n",
    "    # Extract terms and operators\n",
    "    terms = []\n",
    "    operators = []\n",
    "    \n",
    "    for i, part in enumerate(parts):\n",
    "        if part.strip() in ['AND', 'OR']:\n",
    "            operators.append(part.strip())\n",
    "        else:\n",
    "            # Tokenize each term part\n",
    "            term_tokens = tokenize_document(part)\n",
    "            if term_tokens:\n",
    "                terms.append(term_tokens)\n",
    "    \n",
    "    # If no operators found, treat as single term query\n",
    "    if not operators:\n",
    "        if len(terms) == 1:\n",
    "            # Single term query\n",
    "            term = terms[0][0] if terms[0] else \"\"\n",
    "            return inverted_index.get(term, [])\n",
    "        else:\n",
    "            # Multiple terms without operators - treat as AND\n",
    "            operators = ['AND'] * (len(terms) - 1)\n",
    "    \n",
    "    # Get document sets for each term\n",
    "    doc_sets = []\n",
    "    for term_list in terms:\n",
    "        if not term_list:\n",
    "            continue\n",
    "        # For multi-word terms, use the first word (you could extend this)\n",
    "        term = term_list[0]\n",
    "        doc_set = set(inverted_index.get(term, []))\n",
    "        doc_sets.append(doc_set)\n",
    "    \n",
    "    if not doc_sets:\n",
    "        return []\n",
    "    \n",
    "    # Apply Boolean operations\n",
    "    result_set = doc_sets[0]  # Start with first term's documents\n",
    "    \n",
    "    for i, operator in enumerate(operators):\n",
    "        if i + 1 < len(doc_sets):\n",
    "            next_set = doc_sets[i + 1]\n",
    "            \n",
    "            if operator == 'AND':\n",
    "                result_set = result_set.intersection(next_set)\n",
    "            elif operator == 'OR':\n",
    "                result_set = result_set.union(next_set)\n",
    "    \n",
    "    # Convert back to sorted list\n",
    "    return sorted(list(result_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf47585",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "boolean_queries = [\n",
    "  \"gas AND pressure\",\n",
    "  \"structural AND aeroelastic AND flight AND high AND speed OR aircraft\",\n",
    "  \"heat AND conduction AND composite AND slabs\",\n",
    "  \"boundary AND layer AND control\",\n",
    "  \"compressible AND flow AND nozzle\",\n",
    "  \"combustion AND chamber AND injection\",\n",
    "  \"laminar AND turbulent AND transition\",\n",
    "  \"fatigue AND crack AND growth\",\n",
    "  \"wing AND tip AND vortices\",\n",
    "  \"propulsion AND efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf286d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [27, 49, 85, 101, 110]\n",
      "Q2 => [12, 14, 29, 47, 51]\n",
      "Q3 => [5, 399]\n",
      "Q4 => [1, 61, 244, 265, 342]\n",
      "Q5 => [118, 131]\n",
      "Q6 => []\n",
      "Q7 => [7, 9, 80, 89, 96]\n",
      "Q8 => []\n",
      "Q9 => [675]\n",
      "Q10 => [968]\n"
     ]
    }
   ],
   "source": [
    "# Run Boolean queries in batch, using the function you created\n",
    "def run_batch_boolean(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = boolean_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "boolean_results = run_batch_boolean(boolean_queries)\n",
    "for qid, res in boolean_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b591b81",
   "metadata": {},
   "source": [
    "## Part 1.2 – TF-IDF Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed448f",
   "metadata": {},
   "source": [
    "\n",
    "$tf_{i,j} = \\text{Raw Frequency}$\n",
    "\n",
    "$idf_t = \\log\\left(\\frac{N}{df_t}\\right)$\n",
    "\n",
    "### Build document–term matrix (TF and IDF weights)\n",
    "\n",
    "Compute tf and idf using the formulas above and store the weights in a document–term matrix (rows = documents, columns = terms).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "629e32fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating document-term matrix:\n",
      "- Number of documents: 1400\n",
      "- Number of terms: 6934\n",
      "- Matrix size: 1400 x 6934\n",
      "\n",
      "Calculating TF (Term Frequency)...\n",
      "Calculating IDF (Inverse Document Frequency)...\n",
      "Calculating TF-IDF weights...\n",
      "\n",
      "Document-term matrix created successfully!\n",
      "Matrix shape: (1400, 6934)\n",
      "Matrix statistics:\n",
      "- Non-zero entries: 90,604\n",
      "- Total entries: 9,707,600\n",
      "- Sparsity: 0.9907 (99.07% zeros)\n",
      "\n",
      "TF-IDF statistics:\n",
      "- Min TF-IDF value: 0.0000\n",
      "- Max TF-IDF value: 104.4755\n",
      "- Mean TF-IDF value: 0.0456\n",
      "\n",
      "IDF statistics:\n",
      "- Min IDF value: 0.6903\n",
      "- Max IDF value: 7.2442\n",
      "- Mean IDF value: 5.9705\n",
      "\n",
      "Terms with highest IDF (most discriminative):\n",
      "'ab': IDF = 7.2442\n",
      "'abbreviated': IDF = 7.2442\n",
      "'ablated': IDF = 7.2442\n",
      "'ablative': IDF = 7.2442\n",
      "'absent': IDF = 7.2442\n",
      "\n",
      "Terms with lowest IDF (most common):\n",
      "'boundary': IDF = 1.1152\n",
      "'number': IDF = 1.0621\n",
      "'pressure': IDF = 0.9904\n",
      "'results': IDF = 0.8540\n",
      "'flow': IDF = 0.6903\n",
      "\n",
      "Document-term matrix ready for TF-IDF retrieval!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate the weights for the documents and the terms using tf and idf weighting. Put these values into a document–term matrix (rows = documents, columns = terms).\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Get all document IDs and create mappings\n",
    "doc_ids = sorted(list(docs.keys()))\n",
    "term_ids = sorted(list(vocabulary_list))\n",
    "\n",
    "# Create mappings for efficient lookup\n",
    "doc_to_idx = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "term_to_idx = {term: idx for idx, term in enumerate(term_ids)}\n",
    "\n",
    "print(f\"Creating document-term matrix:\")\n",
    "print(f\"- Number of documents: {len(doc_ids)}\")\n",
    "print(f\"- Number of terms: {len(term_ids)}\")\n",
    "print(f\"- Matrix size: {len(doc_ids)} x {len(term_ids)}\")\n",
    "\n",
    "# Initialize the document-term matrix\n",
    "# Rows = documents, Columns = terms\n",
    "doc_term_matrix = np.zeros((len(doc_ids), len(term_ids)))\n",
    "\n",
    "# Calculate TF (Term Frequency) for each document-term pair\n",
    "print(\"\\nCalculating TF (Term Frequency)...\")\n",
    "for doc_id, terms in tokenized_docs.items():\n",
    "    doc_idx = doc_to_idx[doc_id]\n",
    "    \n",
    "    # Count term frequencies in this document\n",
    "    term_counts = {}\n",
    "    for term in terms:\n",
    "        term_counts[term] = term_counts.get(term, 0) + 1\n",
    "    \n",
    "    # Store TF values in matrix\n",
    "    for term, count in term_counts.items():\n",
    "        if term in term_to_idx:\n",
    "            term_idx = term_to_idx[term]\n",
    "            doc_term_matrix[doc_idx, term_idx] = count  # Raw frequency\n",
    "\n",
    "# Calculate IDF (Inverse Document Frequency) for each term\n",
    "print(\"Calculating IDF (Inverse Document Frequency)...\")\n",
    "N = len(doc_ids)  # Total number of documents\n",
    "idf_values = {}\n",
    "\n",
    "for term in term_ids:\n",
    "    # Count how many documents contain this term\n",
    "    df_t = len(inverted_index.get(term, []))  # Document frequency\n",
    "    \n",
    "    if df_t > 0:\n",
    "        # IDF formula: log(N / df_t)\n",
    "        idf_values[term] = math.log(N / df_t)\n",
    "    else:\n",
    "        idf_values[term] = 0  # Term doesn't appear in any document\n",
    "\n",
    "# Calculate TF-IDF weights\n",
    "print(\"Calculating TF-IDF weights...\")\n",
    "tfidf_matrix = np.zeros((len(doc_ids), len(term_ids)))\n",
    "\n",
    "for doc_idx in range(len(doc_ids)):\n",
    "    for term_idx in range(len(term_ids)):\n",
    "        term = term_ids[term_idx]\n",
    "        tf = doc_term_matrix[doc_idx, term_idx]  # Term frequency\n",
    "        idf = idf_values[term]  # Inverse document frequency\n",
    "        tfidf_matrix[doc_idx, term_idx] = tf * idf\n",
    "\n",
    "# Report statistics\n",
    "print(f\"\\nDocument-term matrix created successfully!\")\n",
    "print(f\"Matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Show some statistics\n",
    "non_zero_entries = np.count_nonzero(tfidf_matrix)\n",
    "total_entries = tfidf_matrix.size\n",
    "sparsity = 1 - (non_zero_entries / total_entries)\n",
    "\n",
    "print(f\"Matrix statistics:\")\n",
    "print(f\"- Non-zero entries: {non_zero_entries:,}\")\n",
    "print(f\"- Total entries: {total_entries:,}\")\n",
    "print(f\"- Sparsity: {sparsity:.4f} ({sparsity*100:.2f}% zeros)\")\n",
    "\n",
    "# Show TF-IDF statistics\n",
    "print(f\"\\nTF-IDF statistics:\")\n",
    "print(f\"- Min TF-IDF value: {np.min(tfidf_matrix):.4f}\")\n",
    "print(f\"- Max TF-IDF value: {np.max(tfidf_matrix):.4f}\")\n",
    "print(f\"- Mean TF-IDF value: {np.mean(tfidf_matrix):.4f}\")\n",
    "\n",
    "# Show IDF statistics\n",
    "idf_list = list(idf_values.values())\n",
    "print(f\"\\nIDF statistics:\")\n",
    "print(f\"- Min IDF value: {min(idf_list):.4f}\")\n",
    "print(f\"- Max IDF value: {max(idf_list):.4f}\")\n",
    "print(f\"- Mean IDF value: {sum(idf_list)/len(idf_list):.4f}\")\n",
    "\n",
    "# Show examples of high and low IDF terms\n",
    "sorted_idf = sorted(idf_values.items(), key=lambda x: x[1], reverse=True)\n",
    "print(f\"\\nTerms with highest IDF (most discriminative):\")\n",
    "for term, idf in sorted_idf[:5]:\n",
    "    print(f\"'{term}': IDF = {idf:.4f}\")\n",
    "\n",
    "print(f\"\\nTerms with lowest IDF (most common):\")\n",
    "for term, idf in sorted_idf[-5:]:\n",
    "    print(f\"'{term}': IDF = {idf:.4f}\")\n",
    "\n",
    "print(f\"\\nDocument-term matrix ready for TF-IDF retrieval!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007cbf7",
   "metadata": {},
   "source": [
    "### Build TF–IDF document vectors\n",
    "\n",
    "From the matrix, build a TF–IDF vector for each document (consider normalization if needed for cosine similarity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "654b0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TF-IDF document vectors...\n",
      "Created 1400 TF-IDF document vectors\n",
      "\n",
      "Verifying vector normalization:\n",
      "- Min vector norm: 0.000000\n",
      "- Max vector norm: 1.000000\n",
      "- Mean vector norm: 0.998571\n",
      "\n",
      "Example document vectors:\n",
      "Document 1:\n",
      "  - Non-zero elements: 60/6934\n",
      "  - Max TF-IDF value: 0.5861\n",
      "  - Term with max value: 'slipstream'\n",
      "  - Vector norm: 1.000000\n",
      "Document 2:\n",
      "  - Non-zero elements: 71/6934\n",
      "  - Max TF-IDF value: 0.3222\n",
      "  - Term with max value: 'past'\n",
      "  - Vector norm: 1.000000\n",
      "Document 3:\n",
      "  - Non-zero elements: 14/6934\n",
      "  - Max TF-IDF value: 0.4357\n",
      "  - Term with max value: 'past'\n",
      "  - Vector norm: 1.000000\n",
      "\n",
      "Testing query vector creation:\n",
      "Query: ['aircraft', 'flight', 'pressure']\n",
      "Query vector norm: 1.000000\n",
      "Non-zero elements: 3\n",
      "Query term TF-IDF values:\n",
      "  'aircraft': TF-IDF = 0.7604, IDF = 2.9815\n",
      "  'flight': TF-IDF = 0.5984, IDF = 2.3464\n",
      "  'pressure': TF-IDF = 0.2526, IDF = 0.9904\n",
      "\n",
      "TF-IDF document vectors ready for cosine similarity calculations!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Build TF–IDF document vectors from the document–term matrix\n",
    "\n",
    "# Build TF-IDF document vectors with normalization for cosine similarity\n",
    "print(\"Building TF-IDF document vectors...\")\n",
    "\n",
    "# Create normalized TF-IDF vectors for each document\n",
    "# Normalization is important for cosine similarity calculations\n",
    "tfidf_vectors = {}\n",
    "\n",
    "for doc_idx in range(len(doc_ids)):\n",
    "    doc_id = doc_ids[doc_idx]\n",
    "    \n",
    "    # Get the TF-IDF vector for this document (row from the matrix)\n",
    "    doc_vector = tfidf_matrix[doc_idx, :]\n",
    "    \n",
    "    # Calculate the L2 norm (Euclidean norm) for normalization\n",
    "    l2_norm = np.linalg.norm(doc_vector)\n",
    "    \n",
    "    # Normalize the vector (unit vector for cosine similarity)\n",
    "    if l2_norm > 0:\n",
    "        normalized_vector = doc_vector / l2_norm\n",
    "    else:\n",
    "        # Handle case where document has no terms (shouldn't happen with our data)\n",
    "        normalized_vector = doc_vector\n",
    "    \n",
    "    tfidf_vectors[doc_id] = normalized_vector\n",
    "\n",
    "print(f\"Created {len(tfidf_vectors)} TF-IDF document vectors\")\n",
    "\n",
    "# Verify normalization (all vectors should have unit length)\n",
    "print(\"\\nVerifying vector normalization:\")\n",
    "norms = []\n",
    "for doc_id, vector in tfidf_vectors.items():\n",
    "    norm = np.linalg.norm(vector)\n",
    "    norms.append(norm)\n",
    "\n",
    "print(f\"- Min vector norm: {min(norms):.6f}\")\n",
    "print(f\"- Max vector norm: {max(norms):.6f}\")\n",
    "print(f\"- Mean vector norm: {np.mean(norms):.6f}\")\n",
    "\n",
    "# Show some examples of document vectors\n",
    "print(f\"\\nExample document vectors:\")\n",
    "example_docs = list(doc_ids)[:3]\n",
    "for doc_id in example_docs:\n",
    "    vector = tfidf_vectors[doc_id]\n",
    "    non_zero_count = np.count_nonzero(vector)\n",
    "    max_value = np.max(vector)\n",
    "    max_term_idx = np.argmax(vector)\n",
    "    max_term = term_ids[max_term_idx] if max_term_idx < len(term_ids) else \"unknown\"\n",
    "    \n",
    "    print(f\"Document {doc_id}:\")\n",
    "    print(f\"  - Non-zero elements: {non_zero_count}/{len(vector)}\")\n",
    "    print(f\"  - Max TF-IDF value: {max_value:.4f}\")\n",
    "    print(f\"  - Term with max value: '{max_term}'\")\n",
    "    print(f\"  - Vector norm: {np.linalg.norm(vector):.6f}\")\n",
    "\n",
    "# Create a function to get TF-IDF vector for any document\n",
    "def get_document_vector(doc_id):\n",
    "    \"\"\"\n",
    "    Get the normalized TF-IDF vector for a document.\n",
    "    \n",
    "    Args:\n",
    "        doc_id: Document ID\n",
    "        \n",
    "    Returns:\n",
    "        Normalized TF-IDF vector (numpy array)\n",
    "    \"\"\"\n",
    "    return tfidf_vectors.get(doc_id, np.zeros(len(term_ids)))\n",
    "\n",
    "# Create a function to get TF-IDF vector for a query\n",
    "def get_query_vector(query_terms):\n",
    "    \"\"\"\n",
    "    Get the normalized TF-IDF vector for a query.\n",
    "    \n",
    "    Args:\n",
    "        query_terms: List of query terms\n",
    "        \n",
    "    Returns:\n",
    "        Normalized TF-IDF vector (numpy array)\n",
    "    \"\"\"\n",
    "    # Initialize query vector\n",
    "    query_vector = np.zeros(len(term_ids))\n",
    "    \n",
    "    # Count term frequencies in query\n",
    "    term_counts = {}\n",
    "    for term in query_terms:\n",
    "        term_counts[term] = term_counts.get(term, 0) + 1\n",
    "    \n",
    "    # Calculate TF-IDF for query terms\n",
    "    for term, count in term_counts.items():\n",
    "        if term in term_to_idx:\n",
    "            term_idx = term_to_idx[term]\n",
    "            tf = count  # Raw frequency in query\n",
    "            idf = idf_values.get(term, 0)  # IDF from collection\n",
    "            query_vector[term_idx] = tf * idf\n",
    "    \n",
    "    # Normalize the query vector\n",
    "    l2_norm = np.linalg.norm(query_vector)\n",
    "    if l2_norm > 0:\n",
    "        query_vector = query_vector / l2_norm\n",
    "    \n",
    "    return query_vector\n",
    "\n",
    "# Test the query vector function\n",
    "print(f\"\\nTesting query vector creation:\")\n",
    "test_query = [\"aircraft\", \"flight\", \"pressure\"]\n",
    "query_vec = get_query_vector(test_query)\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Query vector norm: {np.linalg.norm(query_vec):.6f}\")\n",
    "print(f\"Non-zero elements: {np.count_nonzero(query_vec)}\")\n",
    "\n",
    "# Show which terms in the query have non-zero TF-IDF values\n",
    "print(f\"Query term TF-IDF values:\")\n",
    "for term in test_query:\n",
    "    if term in term_to_idx:\n",
    "        term_idx = term_to_idx[term]\n",
    "        tfidf_val = query_vec[term_idx]\n",
    "        idf_val = idf_values.get(term, 0)\n",
    "        print(f\"  '{term}': TF-IDF = {tfidf_val:.4f}, IDF = {idf_val:.4f}\")\n",
    "\n",
    "print(f\"\\nTF-IDF document vectors ready for cosine similarity calculations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df320c",
   "metadata": {},
   "source": [
    "### Implement cosine similarity\n",
    "\n",
    "Implement a function to compute cosine similarity scores between a (tokenized) query and all documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d2e7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing TF-IDF retrieval function:\n",
      "==================================================\n",
      "Query: 'aircraft flight'\n",
      "Results: 1400 documents\n",
      "Top 5 documents: [51, 1169, 253, 810, 1163]\n",
      "Top 5 similarity scores:\n",
      "  Doc 51: 0.3891\n",
      "  Doc 1169: 0.3851\n",
      "  Doc 253: 0.3224\n",
      "  Doc 810: 0.2853\n",
      "  Doc 1163: 0.2660\n",
      "\n",
      "Query: 'pressure gas flow'\n",
      "Results: 1400 documents\n",
      "Top 5 documents: [169, 167, 1286, 665, 166]\n",
      "Top 5 similarity scores:\n",
      "  Doc 169: 0.3194\n",
      "  Doc 167: 0.3177\n",
      "  Doc 1286: 0.2904\n",
      "  Doc 665: 0.2878\n",
      "  Doc 166: 0.2830\n",
      "\n",
      "Query: 'structural aeroelastic analysis'\n",
      "Results: 1400 documents\n",
      "Top 5 documents: [875, 12, 184, 746, 781]\n",
      "Top 5 similarity scores:\n",
      "  Doc 875: 0.3613\n",
      "  Doc 12: 0.3596\n",
      "  Doc 184: 0.2800\n",
      "  Doc 746: 0.2583\n",
      "  Doc 781: 0.2513\n",
      "\n",
      "Similarity score analysis for query 'aircraft flight':\n",
      "- Min similarity: 0.0000\n",
      "- Max similarity: 0.3891\n",
      "- Mean similarity: 0.0102\n",
      "- Documents with similarity > 0: 183\n",
      "- Documents with similarity > 0.1: 47\n",
      "\n",
      "TF-IDF retrieval function implemented successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: Create a function for calculating the similarity score of all the documents by their relevance to query terms\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        vec1, vec2: Normalized vectors (numpy arrays)\n",
    "        \n",
    "    Returns:\n",
    "        Cosine similarity score (float between 0 and 1)\n",
    "    \"\"\"\n",
    "    # Since vectors are normalized, cosine similarity = dot product\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "def tfidf_retrieve(query: str):\n",
    "    \"\"\"\n",
    "    Retrieve documents using TF-IDF and cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Query string\n",
    "        \n",
    "    Returns:\n",
    "        List of document IDs sorted by relevance (highest similarity first)\n",
    "    \"\"\"\n",
    "    # Tokenize the query\n",
    "    query_terms = tokenize_document(query)\n",
    "    \n",
    "    if not query_terms:\n",
    "        return []\n",
    "    \n",
    "    # Get the query vector\n",
    "    query_vector = get_query_vector(query_terms)\n",
    "    \n",
    "    # If query vector is all zeros (no valid terms), return empty results\n",
    "    if np.all(query_vector == 0):\n",
    "        return []\n",
    "    \n",
    "    # Calculate cosine similarity between query and all documents\n",
    "    similarities = []\n",
    "    \n",
    "    for doc_id in doc_ids:\n",
    "        doc_vector = get_document_vector(doc_id)\n",
    "        similarity = cosine_similarity(query_vector, doc_vector)\n",
    "        similarities.append((doc_id, similarity))\n",
    "    \n",
    "    # Sort by similarity score (descending order)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return only document IDs (sorted by relevance)\n",
    "    return [doc_id for doc_id, _ in similarities]\n",
    "\n",
    "# Test the TF-IDF retrieval function\n",
    "print(\"Testing TF-IDF retrieval function:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with a simple query\n",
    "test_query1 = \"aircraft flight\"\n",
    "result1 = tfidf_retrieve(test_query1)\n",
    "print(f\"Query: '{test_query1}'\")\n",
    "print(f\"Results: {len(result1)} documents\")\n",
    "print(f\"Top 5 documents: {result1[:5]}\")\n",
    "\n",
    "# Show similarity scores for top results\n",
    "query_vec1 = get_query_vector(tokenize_document(test_query1))\n",
    "print(f\"Top 5 similarity scores:\")\n",
    "for i, doc_id in enumerate(result1[:5]):\n",
    "    doc_vec = get_document_vector(doc_id)\n",
    "    sim_score = cosine_similarity(query_vec1, doc_vec)\n",
    "    print(f\"  Doc {doc_id}: {sim_score:.4f}\")\n",
    "\n",
    "# Test with another query\n",
    "test_query2 = \"pressure gas flow\"\n",
    "result2 = tfidf_retrieve(test_query2)\n",
    "print(f\"\\nQuery: '{test_query2}'\")\n",
    "print(f\"Results: {len(result2)} documents\")\n",
    "print(f\"Top 5 documents: {result2[:5]}\")\n",
    "\n",
    "# Show similarity scores for top results\n",
    "query_vec2 = get_query_vector(tokenize_document(test_query2))\n",
    "print(f\"Top 5 similarity scores:\")\n",
    "for i, doc_id in enumerate(result2[:5]):\n",
    "    doc_vec = get_document_vector(doc_id)\n",
    "    sim_score = cosine_similarity(query_vec2, doc_vec)\n",
    "    print(f\"  Doc {doc_id}: {sim_score:.4f}\")\n",
    "\n",
    "# Test with a more complex query\n",
    "test_query3 = \"structural aeroelastic analysis\"\n",
    "result3 = tfidf_retrieve(test_query3)\n",
    "print(f\"\\nQuery: '{test_query3}'\")\n",
    "print(f\"Results: {len(result3)} documents\")\n",
    "print(f\"Top 5 documents: {result3[:5]}\")\n",
    "\n",
    "# Show similarity scores for top results\n",
    "query_vec3 = get_query_vector(tokenize_document(test_query3))\n",
    "print(f\"Top 5 similarity scores:\")\n",
    "for i, doc_id in enumerate(result3[:5]):\n",
    "    doc_vec = get_document_vector(doc_id)\n",
    "    sim_score = cosine_similarity(query_vec3, doc_vec)\n",
    "    print(f\"  Doc {doc_id}: {sim_score:.4f}\")\n",
    "\n",
    "# Analyze the distribution of similarity scores\n",
    "print(f\"\\nSimilarity score analysis for query '{test_query1}':\")\n",
    "all_scores = []\n",
    "for doc_id in doc_ids:\n",
    "    doc_vec = get_document_vector(doc_id)\n",
    "    sim_score = cosine_similarity(query_vec1, doc_vec)\n",
    "    all_scores.append(sim_score)\n",
    "\n",
    "all_scores = np.array(all_scores)\n",
    "print(f\"- Min similarity: {np.min(all_scores):.4f}\")\n",
    "print(f\"- Max similarity: {np.max(all_scores):.4f}\")\n",
    "print(f\"- Mean similarity: {np.mean(all_scores):.4f}\")\n",
    "print(f\"- Documents with similarity > 0: {np.count_nonzero(all_scores)}\")\n",
    "print(f\"- Documents with similarity > 0.1: {np.sum(all_scores > 0.1)}\")\n",
    "\n",
    "print(f\"\\nTF-IDF retrieval function implemented successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4968750",
   "metadata": {
    "deletable": false,
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Do not change this code\n",
    "tfidf_queries = [\n",
    "  \"gas pressure\",\n",
    "  \"structural aeroelastic flight high speed aircraft\",\n",
    "  \"heat conduction composite slabs\",\n",
    "  \"boundary layer control\",\n",
    "  \"compressible flow nozzle\",\n",
    "  \"combustion chamber injection\",\n",
    "  \"laminar turbulent transition\",\n",
    "  \"fatigue crack growth\",\n",
    "  \"wing tip vortices\",\n",
    "  \"propulsion efficiency\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18861681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 => [169, 1286, 167, 185, 1003]\n",
      "Q2 => [12, 51, 746, 875, 884]\n",
      "Q3 => [399, 144, 485, 5, 181]\n",
      "Q4 => [368, 748, 638, 451, 1349]\n",
      "Q5 => [389, 118, 1187, 172, 173]\n",
      "Q6 => [974, 628, 397, 308, 635]\n",
      "Q7 => [418, 1264, 315, 272, 9]\n",
      "Q8 => [768, 726, 1196, 883, 884]\n",
      "Q9 => [1284, 433, 675, 1271, 288]\n",
      "Q10 => [968, 1328, 1380, 1092, 592]\n"
     ]
    }
   ],
   "source": [
    "# Run TF-IDF queries in batch (print top-5 results for each), using the function you created\n",
    "def run_batch_tfidf(queries):\n",
    "    results = {}\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        res = tfidf_retrieve(q)\n",
    "        results[f\"Q{i}\"] = res\n",
    "    return results\n",
    "\n",
    "tfidf_results = run_batch_tfidf(tfidf_queries)\n",
    "\n",
    "for qid, res in tfidf_results.items():\n",
    "    print(qid, \"=>\", res[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0989101",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1.3 – Conceptual Questions\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "**1. What is the difference between data retrieval and information retrieval?**\n",
    "*Your answer here*\n",
    "\n",
    "**For the following scenarios, which approach would be suitable data retrieval or information retrieval? Explain your reasoning.** <br>\n",
    "1.a A clerk in pharmacy uses the following query: Medicine_name = Ibuprofen_400mg\n",
    "*Your answer here*\n",
    "\n",
    "1.b A clerk in pharmacy uses the following query: An anti-biotic medicine \n",
    "*Your answer here*\n",
    "\n",
    "1.c Searching for the schedule of a flight using the following query: Flight_ID = ZEFV2\n",
    "*Your answer here*\n",
    "\n",
    "1.d Searching an E-commerce website using the following query to find an specific shoe: Brooks Ghost 15\n",
    "*Your answer here*\n",
    "\n",
    "1.e Searching the same E-commerce website using the following query: Nice running shoes\n",
    "*Your answer here*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
